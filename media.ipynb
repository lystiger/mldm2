{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c452bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (2.4.2)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.13.0.92)\n",
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.12/site-packages (0.10.32)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py~=2.3 in ./venv/lib/python3.12/site-packages (from mediapipe) (2.4.0)\n",
      "Requirement already satisfied: sounddevice~=0.5 in ./venv/lib/python3.12/site-packages (from mediapipe) (0.5.5)\n",
      "Requirement already satisfied: flatbuffers~=25.9 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.12.19)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.12/site-packages (from mediapipe) (4.13.0.92)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from mediapipe) (3.10.8)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.17.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: cffi in ./venv/lib/python3.12/site-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.3.2)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi->sounddevice~=0.5->mediapipe) (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy opencv-python mediapipe scikit-learn joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee31f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9787bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hand landmarker model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1772033904.758178   33806 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1772033904.783634   33803 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'Data/hand_landmarker.task'\n",
    "MODEL_URL = 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task'\n",
    "\n",
    "os.makedirs('Data', exist_ok=True)\n",
    "if not os.path.isfile(MODEL_PATH):\n",
    "    print('Downloading hand landmarker model...')\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.6,\n",
    "    min_hand_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.6,\n",
    ")\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "HAND_CONNECTIONS = [\n",
    "    (0,1),(1,2),(2,3),(3,4),\n",
    "    (0,5),(5,6),(6,7),(7,8),\n",
    "    (5,9),(9,10),(10,11),(11,12),\n",
    "    (9,13),(13,14),(14,15),(15,16),\n",
    "    (13,17),(17,18),(18,19),(19,20),\n",
    "    (0,17)\n",
    "]\n",
    "\n",
    "def draw_hand(frame, hand_landmarks):\n",
    "    h, w = frame.shape[:2]\n",
    "    pts = []\n",
    "    for lm in hand_landmarks:\n",
    "        x = int(lm.x * w)\n",
    "        y = int(lm.y * h)\n",
    "        pts.append((x, y))\n",
    "        cv2.circle(frame, (x, y), 3, (0, 255, 255), -1)\n",
    "\n",
    "    for i, j in HAND_CONNECTIONS:\n",
    "        if i < len(pts) and j < len(pts):\n",
    "            cv2.line(frame, pts[i], pts[j], (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d25cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand(hand_landmarks):\n",
    "    feat = []\n",
    "    for lm in hand_landmarks:\n",
    "        feat.extend([lm.x, lm.y, lm.z])\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b633e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "q : quit\n",
      "c : toggle collect\n",
      "1-9 : set gesture (gesture_1, gesture_2...)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1772033912.921781   33806 landmark_projection_calculator.cc:78] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "QFontDatabase: Cannot find font directory /home/lystiger/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/lystiger/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/lystiger/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/lystiger/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/lystiger/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "current_gesture = 'none'\n",
    "collecting = False\n",
    "records = []\n",
    "\n",
    "print(\"\"\"\n",
    "q : quit\n",
    "c : toggle collect\n",
    "1-9 : set gesture (gesture_1, gesture_2...)\n",
    "\"\"\")\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    frame_idx += 1\n",
    "    ts_ms = int(time.time() * 1000)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "    res = landmarker.detect_for_video(mp_image, ts_ms)\n",
    "\n",
    "    L_feat = [0.0] * 63\n",
    "    R_feat = [0.0] * 63\n",
    "    L_exist = 0\n",
    "    R_exist = 0\n",
    "\n",
    "    if res.hand_landmarks:\n",
    "        for hand_lm, handedness in zip(res.hand_landmarks, res.handedness):\n",
    "            label = handedness[0].category_name\n",
    "            feat = extract_hand(hand_lm)\n",
    "\n",
    "            if label == 'Left':\n",
    "                L_feat = feat\n",
    "                L_exist = 1\n",
    "            else:\n",
    "                R_feat = feat\n",
    "                R_exist = 1\n",
    "\n",
    "            draw_hand(frame, hand_lm)\n",
    "\n",
    "    if collecting:\n",
    "        row = [ts_ms / 1000.0, current_gesture, L_exist, R_exist] + L_feat + R_feat\n",
    "        records.append(row)\n",
    "\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        f'Gesture: {current_gesture} | Collecting: {collecting}',\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7,\n",
    "        (0, 255, 0) if collecting else (0, 0, 255),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    cv2.imshow('Landmark Collector', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        collecting = not collecting\n",
    "        print('Collecting:', collecting)\n",
    "    elif ord('1') <= key <= ord('9'):\n",
    "        current_gesture = f'gesture_{key - ord(\"0\")}'\n",
    "        print('Gesture set:', current_gesture)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "landmarker.close()\n",
    "print('Collected rows:', len(records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba435cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (0, 129)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('Data', exist_ok=True)\n",
    "\n",
    "columns = (\n",
    "    ['ts', 'gesture', 'L_exist', 'R_exist']\n",
    "    + [f'L_{a}{i}' for i in range(21) for a in ['x','y','z']]\n",
    "    + [f'R_{a}{i}' for i in range(21) for a in ['x','y','z']]\n",
    ")\n",
    "\n",
    "if len(records) == 0:\n",
    "    raise ValueError('No records collected. In camera window press c to start collecting, then q to stop.')\n",
    "\n",
    "df = pd.DataFrame(records, columns=columns)\n",
    "out_path = 'Data/hand_landmarks.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print('Saved:', out_path, '| shape =', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a448465",
   "metadata": {},
   "source": [
    "## Train CV Gesture Models (SVM, Random Forest, KNN, Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c70443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b129266",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCannot find hand_landmarks.csv in Data/ or current directory.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoaded:\u001b[39m\u001b[33m'\u001b[39m, dataset_path, \u001b[33m'\u001b[39m\u001b[33m| shape =\u001b[39m\u001b[33m'\u001b[39m, df.shape)\n\u001b[32m     12\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1922\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1919\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1921\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1922\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML2/mldm2/venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:95\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     94\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     99\u001b[39m passed_names = \u001b[38;5;28mself\u001b[39m.names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:575\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "candidate_paths = [\n",
    "    'Data/hand_landmarks.csv',\n",
    "    'hand_landmarks.csv'\n",
    "]\n",
    "\n",
    "dataset_path = next((p for p in candidate_paths if os.path.isfile(p) and os.path.getsize(p) > 0), None)\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        'No non-empty hand_landmarks.csv found. Run collection cell, press c to collect, then run save cell.'\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "if df.empty:\n",
    "    raise ValueError(f'Dataset is empty: {dataset_path}. Collect more samples first.')\n",
    "\n",
    "print('Loaded:', dataset_path, '| shape =', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed745e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_candidates = ['gesture', 'label']\n",
    "label_col = next((c for c in label_candidates if c in df.columns), None)\n",
    "if label_col is None:\n",
    "    raise ValueError('Dataset must contain a label column: gesture or label')\n",
    "\n",
    "drop_cols = {'ts', 'timestamp_s', 'timestamp_ms', 'frame_time', label_col}\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "y = df[label_col].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('Label column:', label_col)\n",
    "print('Feature count:', len(feature_cols))\n",
    "print('Train/Test:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'svm': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42))\n",
    "    ]),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    'knn': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=5))\n",
    "    ]),\n",
    "    'logistic': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    trained_models[name] = model\n",
    "    results.append({'model': name, 'accuracy': acc})\n",
    "\n",
    "    print(f'\\n=== {name.upper()} ===')\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.loc[0, 'model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "bundle = {\n",
    "    'model_name': best_model_name,\n",
    "    'model': best_model,\n",
    "    'feature_cols': feature_cols,\n",
    "    'label_col': label_col\n",
    "}\n",
    "\n",
    "os.makedirs('Data', exist_ok=True)\n",
    "model_path = 'Data/cv_best_model.joblib'\n",
    "joblib.dump(bundle, model_path)\n",
    "print('Saved best model:', best_model_name, '->', model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68054f2",
   "metadata": {},
   "source": [
    "## Live Webcam Prediction With Trained Model\n",
    "Run this after training. Press `q` to quit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = joblib.load('Data/cv_best_model.joblib')\n",
    "model = bundle['model']\n",
    "feature_cols = bundle['feature_cols']\n",
    "print('Loaded model:', bundle['model_name'])\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path='Data/hand_landmarker.task'),\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_hands=2\n",
    ")\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_idx = 0\n",
    "start_ts_ms = int(time.time() * 1000)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    frame_idx += 1\n",
    "    ts_ms = start_ts_ms + frame_idx\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "    res = landmarker.detect_for_video(mp_image, ts_ms)\n",
    "\n",
    "    L_feat = [0.0] * 63\n",
    "    R_feat = [0.0] * 63\n",
    "    L_exist = 0\n",
    "    R_exist = 0\n",
    "\n",
    "    if res.hand_landmarks:\n",
    "        for hand_lm, handedness in zip(res.hand_landmarks, res.handedness):\n",
    "            label = handedness[0].category_name\n",
    "            feat = extract_hand(hand_lm)\n",
    "            if label == 'Left':\n",
    "                L_feat = feat\n",
    "                L_exist = 1\n",
    "            else:\n",
    "                R_feat = feat\n",
    "                R_exist = 1\n",
    "\n",
    "            draw_hand(frame, hand_lm)\n",
    "\n",
    "    sample = {'L_exist': L_exist, 'R_exist': R_exist}\n",
    "\n",
    "    for i in range(21):\n",
    "        sample[f'L_x{i}'] = L_feat[i * 3 + 0]\n",
    "        sample[f'L_y{i}'] = L_feat[i * 3 + 1]\n",
    "        sample[f'L_z{i}'] = L_feat[i * 3 + 2]\n",
    "        sample[f'R_x{i}'] = R_feat[i * 3 + 0]\n",
    "        sample[f'R_y{i}'] = R_feat[i * 3 + 1]\n",
    "        sample[f'R_z{i}'] = R_feat[i * 3 + 2]\n",
    "\n",
    "    x_live = pd.DataFrame([sample])\n",
    "\n",
    "    for c in feature_cols:\n",
    "        if c not in x_live.columns:\n",
    "            x_live[c] = 0.0\n",
    "\n",
    "    x_live = x_live[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "\n",
    "    pred = model.predict(x_live)[0]\n",
    "    conf = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        conf = float(model.predict_proba(x_live).max())\n",
    "\n",
    "    text = f'Pred: {pred}' if conf is None else f'Pred: {pred} ({conf:.2f})'\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('CV Live Prediction', frame)\n",
    "    if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "landmarker.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}