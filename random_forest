import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# ==========================================
# 0. CHUáº¨N Bá»Š Dá»® LIá»†U (Giáº£ láº­p data náº¿u cáº­u chÆ°a cÃ³ file tháº­t)
# ==========================================
# Trong thá»±c táº¿, X_ML vÃ  y_labels lÃ  káº¿t quáº£ tráº£ vá» tá»« hÃ m Feature Engineering lÃºc nÃ£y
# á» Ä‘Ã¢y tá»› táº¡o data giáº£ Ä‘á»ƒ code cÃ³ thá»ƒ cháº¡y ngay láº­p tá»©c
np.random.seed(42)
X_ML = np.random.rand(600, 34) # 600 máº«u, 34 features
y_labels = np.array(['A']*200 + ['B']*200 + ['C']*200)

# Chuyá»ƒn Ä‘á»•i nhÃ£n dáº¡ng chá»¯ (A, B, C) thÃ nh sá»‘ Ä‘á»ƒ tÃ­nh ROC
classes = np.unique(y_labels)
y_bin = label_binarize(y_labels, classes=classes)
n_classes = y_bin.shape[1]

# Chia táº­p Train (80%) vÃ  Test (20%)
X_train, X_test, y_train, y_test = train_test_split(X_ML, y_labels, test_size=0.2, random_state=42)
_, _, y_train_bin, y_test_bin = train_test_split(X_ML, y_bin, test_size=0.2, random_state=42)

# ==========================================
# 1. TRAIN MÃ” HÃŒNH RANDOM FOREST
# ==========================================
print("â³ Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest...")
# warm_start=True giÃºp ta váº½ Ä‘Æ°á»£c OOB Error qua tá»«ng bÆ°á»›c thÃªm cÃ¢y
rf_model = RandomForestClassifier(n_estimators=15, warm_start=True, oob_score=True, random_state=42)

# Máº£ng lÆ°u trá»¯ OOB Error (thay tháº¿ cho Loss Curve)
oob_errors = []

# MÃ´ phá»ng quÃ¡ trÃ¬nh thÃªm dáº§n tá»«ng cÃ¢y vÃ o rá»«ng (tá»« 15 Ä‘áº¿n 150 cÃ¢y)
min_estimators = 15
max_estimators = 150
for i in range(min_estimators, max_estimators + 1):
    rf_model.set_params(n_estimators=i)
    rf_model.fit(X_train, y_train)
    # OOB Error = 1 - OOB Score
    oob_error = 1 - rf_model.oob_score_
    oob_errors.append((i, oob_error))

# Chá»‘t láº¡i model dÃ¹ng Ä‘á»ƒ test
y_pred = rf_model.predict(X_test)
# --- TÃŒM VÃ€ IN RA CÃC MáºªU Bá»Š NHáº¦M LáºªN ---
# Lá»c ra cÃ¡c vá»‹ trÃ­ mÃ  Thá»±c táº¿ (y_test) khÃ¡c vá»›i Dá»± Ä‘oÃ¡n (y_pred)
errors_index = np.where(y_test != y_pred)[0]

print(f"\nğŸ” TÃŒM THáº¤Y {len(errors_index)} MáºªU ÄOÃN SAI:")
for idx in errors_index:
    actual = y_test[idx]
    predicted = y_pred[idx]
    
    # Láº¥y ra bá»™ Ä‘áº·c trÆ°ng cá»§a máº«u bá»‹ sai Ä‘Ã³ Ä‘á»ƒ soi xÃ©t (chá»‰ in vÃ i thÃ´ng sá»‘ chÃ­nh cho gá»n)
    # VÃ­ dá»¥: Láº¥y 5 giÃ¡ trá»‹ Flex trung bÃ¬nh (náº±m á»Ÿ 5 cá»™t Ä‘áº§u tiÃªn cá»§a X_test)
    flex_vals = X_test[idx][:5] 
    
    print(f"- Láº½ ra lÃ  chá»¯ [{actual}] nhÆ°ng Model láº¡i Ä‘oÃ¡n lÃ  [{predicted}].")
    print(f"  GiÃ¡ trá»‹ Flex sensor lÃºc Ä‘Ã³: {np.round(flex_vals, 2)}")
y_score = rf_model.predict_proba(X_test) # XÃ¡c suáº¥t cho ROC

# ==========================================
# 2. IN ACCURACY
# ==========================================
acc = accuracy_score(y_test, y_pred)
print(f"\nâœ… HUáº¤N LUYá»†N XONG!")
print(f"ğŸ¯ Äá»™ chÃ­nh xÃ¡c (Accuracy) trÃªn táº­p Test: {acc * 100:.2f}%")

# ==========================================
# 3. Váº¼ BIá»‚U Äá»’ Tá»”NG Há»¢P (1 HÃŒNH CHá»¨A 3 BIá»‚U Äá»’)
# ==========================================
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# --- Biá»ƒu Ä‘á»“ 1: OOB Error Curve (Thay cho Loss) ---

n_trees, errors = zip(*oob_errors)
axes[0].plot(n_trees, errors, color='red', linewidth=2)
axes[0].set_title('OOB Error Rate (Learning Curve)')
axes[0].set_xlabel('Sá»‘ lÆ°á»£ng Decision Trees')
axes[0].set_ylabel('Tá»‰ lá»‡ lá»—i (OOB Error)')
axes[0].grid(True, linestyle='--', alpha=0.7)

# --- Biá»ƒu Ä‘á»“ 2: Confusion Matrix ---

cm = confusion_matrix(y_test, y_pred, labels=classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], 
            xticklabels=classes, yticklabels=classes)
axes[1].set_title('Confusion Matrix')
axes[1].set_xlabel('Dá»± Ä‘oÃ¡n cá»§a Model (Predicted)')
axes[1].set_ylabel('Thá»±c táº¿ (Actual)')

# --- Biá»ƒu Ä‘á»“ 3: ROC Curve (Multi-class) ---

colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])
for i, color in zip(range(n_classes), colors):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    axes[2].plot(fpr, tpr, color=color, lw=2,
                 label=f'Class {classes[i]} (AUC = {roc_auc:.2f})')

axes[2].plot([0, 1], [0, 1], 'k--', lw=2) # ÄÆ°á»ng chÃ©o ngáº«u nhiÃªn
axes[2].set_xlim([0.0, 1.0])
axes[2].set_ylim([0.0, 1.05])
axes[2].set_title('ROC Curve (ÄÆ°á»ng cong Äáº·c trÆ°ng Hoáº¡t Ä‘á»™ng)')
axes[2].set_xlabel('False Positive Rate (FPR)')
axes[2].set_ylabel('True Positive Rate (TPR)')
axes[2].legend(loc="lower right")
axes[2].grid(True, linestyle='--', alpha=0.7)

# Hiá»ƒn thá»‹ táº¥t cáº£
plt.tight_layout()
plt.show()
